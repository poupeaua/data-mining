---
title: "TP2 geneticData Mining - PCA-regression in genetics"
output: html_notebook
---

Lamyaa BOUZBIBA
Alexandre POUPEAU
Eloise JULIEN

### 1. geneticData

```{r}

NAm2 = read.table("NAm2.txt", header = TRUE)

# unique is used to get the name of all population / ethnics without duplicates
names=unique(NAm2$Pop)
npop=length(names)

# coordinates for each pop
coord=unique(NAm2[, c("Pop","long","lat")])
colPalette=rep(c("black","red","cyan","orange","brown","blue","pink","purple","darkgreen"),3)

# type 16 = circle / type 15 = square / type 25 = triangle
pch=rep(c(16,15,25),each=9)

# the display works because for each population we have the latitude and the longitude
plot(coord[, c("long","lat")],pch=pch,col=colPalette,asp=1)

# asp allows to have the correct ratio between  axis  longitude and latitude
# Then the map is not deformed  
legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=2,lwd=2)

library(maps);map("world",add=T)


```

The description of the script is in the commentaries above.

### 2. Regression

```{r}

# turn the matrix into a geneticData frame
geneticData = NAm2[,-c(1:7)]

geneticDataFrame = data.frame(geneticData)

# creation of the linear model
reg = lm(formula = long ~ . , data = geneticDataFrame)

# uncomment that following part to see the summary of the regression
# summary(reg)

```

All values are NA (Not Available) because we have too much predictors. The $X$ matrix is cannot be reversed so we have to use the PCA method to counter that problem.

### 3.PCA Principal Component Analysis

#### a) 

The main goal of the PCA is to reduce the number of predictors. In order to achieve that, we have to extract the most important components of the geneticData set. These new variables are a linear combinaison of the original ones. The number of principal components is smaller than the number of original variables. The aim is to maximize the variation of the geneticData set.

#### b)

```{r}

# install.packages("factoextra")

library(factoextra)

allGenes <- geneticDataFrame[, -1]

pcaNAm2 <- prcomp(allGenes, scale = FALSE)

fviz_eig(pcaNAm2)

vectProportionOfVariance <- pcaNAm2$sdev^2/sum(pcaNAm2$sdev^2)

# summary(pcaNAm2.pca)


```

It is better to use scale = FALSE by convention because we get a better representation of the percentage of explained values.

FALSE : 494 = linear combianison of the 5000 genes
We notice that we get only 494 principal components instead of approximately 5000 which correspond to the number of genes. This can be explained because "" in the covariance matrix the eigenvalues from 495 to ~5000 are equals to zero. In reality, we only select the genes that best explain the model up to 500. If we had to really find the "best-explaining", which means finding the most explicative genes from 1 to 5000 we would have to test all the combinaisons of 494 genes.   

#### c)

```{r}

caxes=c(1,2)
plot(pcaNAm2$x[,caxes],col="white")

for (i in 1:npop) {
  # print(names[i])
  lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],
  type="p",col=colPalette[i],pch=pch[i])
}
legend("topleft",legend=names,col=colPalette,lty=-
1,pch=pch,cex=.65,ncol=3,lwd=2)

caxes=c(5,6)
plot(pcaNAm2$x[,caxes],col="white")

for (i in 1:npop) {
  # print(names[i])
  lines(pcaNAm2$x[which(NAm2[,3]==names[i]),caxes],
  type="p",col=colPalette[i],pch=pch[i])
}
legend("topright",legend=names,col=colPalette,lty=-
1,pch=pch,cex=.65,ncol=3,lwd=2)

```

The Ache population is well identified by both components. The Surui can also be identified by both components but more especially by the PC1.

The Karitiana population is well identified by the PC5. The Pima population is well identified by the PC6.

#### d)


```{r}

inertia = vectProportionOfVariance[1] + vectProportionOfVariance[2]

sprintf("The inertia is equal to %f.", inertia)

```


According to what we have seen until now, we have two ideas.

The first idea is that we have seen that we can distinguish two population using two principal components. Therefore, as there are 27 different populations, we could deduce that we would need 14 pairs of principal components. 

However, we can ask ourselves if we can really each time distinguish two different populations using two principal components. It might not be the case because we have just tested it with two pairs of principal components. Thus, we could think about an alternative way, which is adding the variance of the first principal components until the sum is equal to a certain level. This level could be 25% or 50%. This last one means using a lot of principal components. 

```{r}

numberOfPCs <- function(vectorProportionOfVariance, percentage) {
  tmp = 0.0
  i = 1
  while (tmp < percentage) {
    tmp = tmp + vectProportionOfVariance[i]
    i = i + 1
  }
  return(i)
}

nbrPC25 <- numberOfPCs(vectProportionOfVariance, 0.25)
nbrPC50 <- numberOfPCs(vectProportionOfVariance, 0.5)
nbrPC75 <- numberOfPCs(vectProportionOfVariance, 0.75)
nbrPC99 <- numberOfPCs(vectProportionOfVariance, 0.99)

sprintf("Number of principal components needed in order to cover at least 25 percent of variance : %i", nbrPC25)
sprintf("Number of principal components needed in order to cover at least 50 percent of variance : %i", nbrPC50)
sprintf("Number of principal components needed in order to cover at least 75 percent of variance : %i", nbrPC75)
sprintf("Number of principal components needed in order to cover at least 99 percent of variance : %i", nbrPC99)

```

With the test we have established just before we can see that less than $1/10$  of the data contains $25%$ of the variance : So it seems like a good idea to use only $50$ PC to get most of the information within least data. So we might use that number of PC in order to represent genetic markers.
The other numbers give a idea of the number needed if we want to cover more information.

### 4. PCR Principal Components Regression

#### a)

```{r}

# pcaNAm2$x are scores : size 494 * 494 matrix. We take all the ligns but only 250 columns
first250PCs <- pcaNAm2$x[, c(1:250)]

# size 494 * 251 => first column become the longitude
long <- c(NAm2$long, first250PCs)
longMatrix <- matrix(data = long, nrow = 494, ncol = 251)
dataFrameLong <- data.frame(longMatrix)
#dataFrameLong
lmlong = lm(formula = X1 ~ . , data = dataFrameLong)

# size 494 * 251 => first column become the latitude
lat <- c(NAm2$lat, first250PCs)
latMatrix <- matrix(data = lat, nrow = 494, ncol = 251)
dataFrameLat <- data.frame(latMatrix)
dataFrameLat
lmlat = lm(formula = X1 ~ . , data = dataFrameLat)

plot(lmlong$fitted.values,lmlat$fitted.values,col="white")

for (i in 1:npop) {
  #print(names[i])
  lines(lmlong$fitted.values[which(NAm2[,3]==names[i])],
        lmlat$fitted.values[which(NAm2[,3]==names[i])],
        type="p",col=colPalette[i],pch=pch[i])
}

legend("bottomleft",legend=names,col=colPalette,lty=-1,pch=pch,cex=.75,ncol=3,lwd=2)


```

If we compare it to the map from the question 1, we can clearly say that this map represent very optimistically places where population live. It looks quite messy if we just take a look around Panama. However if we examine the estimated places of the Chipewyan, Cree, Ojibwa, Pima and Huiliche populations, we can very distinctly see that the estimation is pretty accurate.

#### b) 

```{r}

# import the library needed to use rdist.earth
library("fields")

# select the theory latitude and longitude of populations
theoryPosition <- matrix(data = c(NAm2$long, NAm2$lat), nrow = 494, ncol = 2)
#theoryPosition

# vector used to stock the mean of distance for each population
stock <- c()

tmpIndex <- 0
for (i in 1:npop) {
  # vector long estimated for population n째i
  tmpLong <- lmlong$fitted.values[which(NAm2[,3]==names[i])]
  
  # vector lat estimated for population n째i
  tmpLat <- lmlat$fitted.values[which(NAm2[,3]==names[i])]
  
  # matrix of the estimated positions
  estimatedPosition <- matrix(data = c(tmpLong, tmpLat), ncol = 2)
  #print(estimatedPosition)
  
  len <- length(estimatedPosition)/2
  
  # by making unique we assume that two estimated coordinates will never be at the same exact position
  diff <- unique(rdist.earth(x1 = theoryPosition[tmpIndex:(tmpIndex+len), ], x2 = estimatedPosition, miles = F, R = NULL))
  
  # stock the mean of distance from the theorical position in kilometers
  stock[i] <- mean(diff)
  
  tmpIndex <- tmpIndex + len
}

plot(c(1:npop), stock, main = "Mean difference in kilometers between theoric and estimated positions",
     xlab = "N째 associated to the population", 
     ylab = "Mean of the distance in kilometers")
linearTest <- lm(stock ~ c(1:npop))
abline(linearTest)

```

Thanks to the previous graph, we can say that the estimated positions are on average approximately $1100$ kilometers away from the theorical position. It might seem a lot at first sight because it sounds huge if we just have a human sensible approach. Nonetheless it is not really a lot in reality if we size to the scale of the earth. Indeed, we could just think about the fact that the earth's circonference is equal to $40075$ kilometers. If we draw a circle around the theorical position for each population, on average, the estimation of the position will be contained inside the circle.    


### 5) PCR and Cross-Validation

#### a)

Cross-Validation is a technique used in model selection to better estimate the test error of a predictive model. The principle of K-fold cross validation method is to divide randomly the data into K subsets where K-1 subsets are used as the training sets and the remaining subset is used as a validation subset to compute a prediction error. We repeat this procedure K times by considering each subset as the validation subset and the others subsets as the training sets. The procedure is finished when each subset has been used once as the validation subset. The K results from the folds can then be averaged to produce a single estimation.

The advantage of this method is that all observations are used for both training and validation, and each observation is used for validation exactly once.

```{r}

labels <- rep(1:10, each=10)
set <- sample(labels, 494, replace = TRUE)
set

```

#### b)

```{r}

naxes <- 4
predictedCoord <- matrix(data = NA, nrow = 494, ncol = 2)

# Regression for the longitude
pcalong <- data.frame(cbind(long=NAm2[,c("long")], pcaNAm2$x[,1:naxes], set))

reglong <- lm(formula = long ~ . , data = subset(pcalong[, 1:5], set != 1))

# Regression for the latitude
pcalat <- data.frame(cbind(lat=NAm2[,c("lat")], pcaNAm2$x[,1:naxes], set))

reglat <- lm(formula = lat ~ . , data = subset(pcalat[, 1:5], set != 1))


# Prediction
# i = 1
predict_long <- predict(reglong, newdata=data.frame(subset(pcalong, set == 1)))
predict_lat <- predict(reglat, newdata=data.frame(subset(pcalat, set == 1)))

idx = 1
for (indice in 1:494) {
  
  if (pcalong[indice, 6] == 1) {
    
    predictedCoord[indice, 1] = predict_long[idx]
    predictedCoord[indice, 2] = predict_lat[idx]
    idx = idx + 1
  }
  
}

# predictedCoord

```



```{r}

for(i in 2:10){
    
    reglong <- lm(formula = long ~ . , data = subset(pcalong[, 1:5], set != i))
    reglat <- lm(formula = lat ~ . , data = subset(pcalat[, 1:5], set != i))
    
    predict_long <- predict(reglong, newdata=data.frame(subset(pcalong, set == i)))
    predict_lat <- predict(reglat, newdata=data.frame(subset(pcalat, set == i)))
    
    idx = 1
    for (j in 1:494) {
      
      if (pcalong[j, 6] == i){
      
        predictedCoord[j, 1] <- predict_long[idx]
        predictedCoord[j, 2] <- predict_lat[idx]
        idx = idx + 1
        
      }
      
    }
    
}

# predictedCoord


# Calculation of the prediction error

```



```{r}

schtroumph <- rdist.earth(x1 = theoryPosition, x2 = predictedCoord, miles = F, R = NULL)

vect <- schtroumph[1, ]
mean(vect[1:30])

index_vect <- c(1:npop)

indice_main_vect <- 1
index = 1
tmp = NAm2$Pop[1]
for (popName in NAm2$Pop) {
  
  string = popName
  
  if (string != tmp) {
    index_vect[indice_main_vect] <- index
    indice_main_vect <- indice_main_vect + 1
  }
  
  tmp = string
  index = index + 1
}

index_vect[npop] <- 494
main_vect <- c(1:npop)
indice1 <- 1
for (indice in 1:npop) {
  
  indice2 <- index_vect[indice]
  main_vect[indice] <- mean(vect[indice1:indice2])
  indice1 <- indice2
  
}

stock <- main_vect
stock

plot(c(1:npop), stock, main = "Mean difference in kilometers between theoric and estimated positions",
     xlab = "N째 associated to the population", 
     ylab = "Mean of the distance in kilometers")
linearTest <- lm(stock ~ c(1:npop))
abline(linearTest)


```

#### c)

```{r}

for (naxes in 2:440){
  
}

```





